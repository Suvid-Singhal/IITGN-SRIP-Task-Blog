{"title":"Binary and Multi-Class Classification in Python","markdown":{"yaml":{"title":"Binary and Multi-Class Classification in Python","description":"The blog post created using the jupyter notebook file and Quarto","author":"Suvid Singhal","categories":["Deep Learning"],"date":"5/22/2021","draft":false},"headingText":"Selection Task","containsRefs":false,"markdown":"\n\n\nI used an NVIDIA RTX 3050 for this task but the code will work on CPU as well\n\nFulfilled all the requirements by implementing the following:\n- Organized the dataset for one-vs-rest (binary) classification\n- Organized the dataset for 5-class classification\n- Performed binary classification using existing architecture (pre-trained AlexNet)\n- Performed 5-class classification using existing architecture (pre-trained AlexNet)\n- Performed binary classification using existing architecture (AlexNet trained from scratch on the given dataset)\n- Performed 5-class classification using existing architecture (AlexNet trained from scratch on the given dataset)\n- Created a custom model for binary classification and trained it on the given dataset\n- Created a custom model for 5-class classification and trained it on the given dataset\n- Used 3-fold validation for calculating the accuracies of the models\n- Generated classification matrices for the models and also calculated accuracy using it\n- Visualized the output of all the convolutional layers for different classes in the custom CNN model\n- Discussed my insights on the automatically created features by the custom CNN\n\nI was not able to upload pre-trained models on GitHub due to size limit\n\n### Download the entire code and my trained models here:\n\n### [Click Here](https://drive.google.com/drive/folders/1Pb5a-NpP4SW-E9FfP1hIZjBD4k_Gz95t?usp=drive_link)\n\n\nThe directory structure should look like this:\n![directory_structure.png](directory_structure.png)\n\n## Restructuring for One-vs-Rest\n\nThe \"animals\" directory contains 90 different labels and each folder contains 60 images of each animal\n\nFor One-vs-Rest classification, we'll choose bat as our positive sample\n\nThe following code creates the appropriate directory structure, if it does not exist.\n\nThis notebook must be present in the same directory as the \"animal\" directory for the code to work.\n\nSelecting 5 random images from each class to build the \"others\" label.\n\n\n```python\nimport os\nimport shutil\nimport random\nsrc_folder = \"animals/bat\"\nfolders = os.listdir(\"animals\")\ntemp = []\nfor folder in folders:\n    if folder != \"bat\":\n        temp.append([os.path.join(\"animals/\"+folder, file) for file in random.sample(os.listdir(\"animals/\"+folder),1)])\nto_copy=[]\nfor temp_list in temp:\n    for item in temp_list:\n        to_copy.append(item)\nif not os.path.exists(\"one-vs-rest\"):\n    shutil.copytree(src_folder, \"one-vs-rest/bat\")\n    os.mkdir(\"one-vs-rest/others\")\n    for file_name in to_copy:\n        shutil.copy2(file_name, \"one-vs-rest/others/\")\n```\n\n## Using pre-trained existing architecture (AlexNet) for binary classification (One vs Rest)\n\n\n```python\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\nnum_epochs = 10\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load dataset\ndataset = torchvision.datasets.ImageFolder(root='one-vs-rest/', transform=transform)\n\n# Define k-fold cross-validation\nkfold = KFold(n_splits=3, shuffle=True, random_state=42)\n\naccuracies=[]\n\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n    print(f\"Fold {fold+1}\")\n\n    # Split dataset into train and validation subsets\n    train_subset = Subset(dataset, train_idx)\n    val_subset = Subset(dataset, val_idx)\n\n    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n\n    # Load pre-trained AlexNet model\n    model = torchvision.models.alexnet(pretrained=True)\n\n    # Freeze the parameters of the pre-trained layers\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Modify the last fully connected layer for binary classification\n    num_ftrs = model.classifier[6].in_features\n    model.classifier[6] = nn.Linear(num_ftrs, 2)\n\n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Train the model\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f\"Epoch {epoch+1}/{5}, Training Loss: {epoch_loss:.4f}\")\n\n    # Evaluate the model\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    accuracy = correct / total\n    accuracies.append(accuracy)\n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n\n    # Save the model\n    torch.save(model.state_dict(), f'alexnet_ovr_fold_{fold+1}.pth')\n\nprint(\"\\nFinal Mean Accuracy:\", '{0:.4f}'.format(np.mean(accuracies)))\n```\n\n    Fold 1\n    \n\n    C:\\Users\\suvid\\.conda\\envs\\intern\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n      warnings.warn(\n    C:\\Users\\suvid\\.conda\\envs\\intern\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n      warnings.warn(msg)\n    \n\n    Epoch 1/5, Training Loss: 0.4932\n    Epoch 2/5, Training Loss: 0.1409\n    Epoch 3/5, Training Loss: 0.0635\n    Epoch 4/5, Training Loss: 0.0384\n    Epoch 5/5, Training Loss: 0.0199\n    Validation Accuracy: 0.9625\n    Fold 2\n    Epoch 1/5, Training Loss: 0.5235\n    Epoch 2/5, Training Loss: 0.1369\n    Epoch 3/5, Training Loss: 0.0816\n    Epoch 4/5, Training Loss: 0.0291\n    Epoch 5/5, Training Loss: 0.0238\n    Validation Accuracy: 0.8861\n    Fold 3\n    Epoch 1/5, Training Loss: 0.6815\n    Epoch 2/5, Training Loss: 0.2231\n    Epoch 3/5, Training Loss: 0.1060\n    Epoch 4/5, Training Loss: 0.0705\n    Epoch 5/5, Training Loss: 0.0449\n    Validation Accuracy: 0.9620\n    \n    Final Mean Accuracy: 0.9369\n    \n\n\n```python\n#Predicting\n\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Load the trained model\nmodel = torchvision.models.alexnet(pretrained=True)\nnum_features = model.classifier[6].in_features\nmodel.classifier[6] = torch.nn.Linear(num_features, 2)\n\n# Load the saved state dictionary\nstate_dict = torch.load('alexnet_ovr_fold_1.pth')\n\n# Remove the 'module.' prefix if present (for loading from a DataParallel model)\nif 'module.' in list(state_dict.keys())[0]:\n    state_dict = {k[7:]: v for k, v in state_dict.items()}\n\n# Load the state dictionary into the model\nmodel.load_state_dict(state_dict)\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Define transforms for preprocessing the image\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load and preprocess the image\nimage_path = 'C:\\\\Users\\\\suvid\\\\Desktop\\\\fruitbat.jpg'  # Replace with the path to your image\nimage = Image.open(image_path)\nimage = image.convert(\"RGB\")\n\nimage_tensor = transform(image)\nimage_tensor = image_tensor.unsqueeze(0)  # Add batch dimension\n\n# Perform prediction\nwith torch.no_grad():\n    outputs = model(image_tensor)\n    _, predicted = torch.max(outputs, 1)\n    predicted_class = predicted.item()\n\n# Get class label\nclass_labels = ['Bat', 'Others']\npredicted_label = class_labels[predicted_class]\n\n# Visualize the image and prediction\nplt.imshow(image)\nplt.title(f'Predicted Class: {predicted_label}')\nplt.axis('off')\nplt.show()\n\n```\n\n\n    \n![](output_8_0.png)\n    \n\n\n## Restructuring for 5 Class Classification\n\nWe're taking 5 labels: cat, cow, octopus, oyster and panda\n\n\n```python\nimport os\nimport shutil\nimport random\n\nif not os.path.exists(\"five-class\"):\n    shutil.copytree(\"animals/cat\", \"five-class/cat\")\n    shutil.copytree(\"animals/cow\", \"five-class/cow\")\n    shutil.copytree(\"animals/octopus\", \"five-class/octopus\")\n    shutil.copytree(\"animals/oyster\", \"five-class/oyster\")\n    shutil.copytree(\"animals/panda\", \"five-class/panda\")\n\n```\n\n## 5-Class Classification using pre-trained AlexNet\n\n\n```python\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.model_selection import KFold\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load dataset\ndataset = torchvision.datasets.ImageFolder(root='five-class/', transform=transform)\n\n# Define k-fold cross-validation\nkfold = KFold(n_splits=3, shuffle=True, random_state=42)\n\naccuracies=[]\n\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n    print(f\"Fold {fold+1}\")\n\n    # Split dataset into train and validation subsets\n    train_subset = Subset(dataset, train_idx)\n    val_subset = Subset(dataset, val_idx)\n\n    # Create data loaders\n    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n\n    # Load pre-trained AlexNet model\n    model = torchvision.models.alexnet(pretrained=True)\n\n    # Modify the final fully connected layer for 5-class classification\n    num_ftrs = model.classifier[6].in_features\n    model.classifier[6] = nn.Linear(num_ftrs, 5)  # 5 output classes for 5-class classification\n\n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n\n    # Train the model\n    for epoch in range(5):  # Number of epochs\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f\"Epoch {epoch+1}/{5}, Training Loss: {epoch_loss:.4f}\")\n\n    # Evaluate the model\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    accuracy = correct / total\n    accuracies.append(accuracy)\n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n\n    # Save the model\n    torch.save(model.state_dict(), f'alexnet_pretrained_5_class_fold_{fold+1}.pth')\n\nprint(\"\\nFinal Mean Accuracy:\", '{0:.4f}'.format(np.mean(accuracies)))\n```\n\n    Fold 1\n    Epoch 1/5, Training Loss: 2.0340\n    Epoch 2/5, Training Loss: 1.5618\n    Epoch 3/5, Training Loss: 1.2037\n    Epoch 4/5, Training Loss: 0.9442\n    Epoch 5/5, Training Loss: 0.7466\n    Validation Accuracy: 0.8000\n    Fold 2\n    Epoch 1/5, Training Loss: 1.8057\n    Epoch 2/5, Training Loss: 1.3289\n    Epoch 3/5, Training Loss: 0.9609\n    Epoch 4/5, Training Loss: 0.7331\n    Epoch 5/5, Training Loss: 0.5253\n    Validation Accuracy: 0.8100\n    Fold 3\n    Epoch 1/5, Training Loss: 1.7709\n    Epoch 2/5, Training Loss: 1.3511\n    Epoch 3/5, Training Loss: 0.9841\n    Epoch 4/5, Training Loss: 0.7640\n    Epoch 5/5, Training Loss: 0.5536\n    Validation Accuracy: 0.7900\n    \n    Final Mean Accuracy: 0.8000\n    \n\n\n```python\n#Predicting\n\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Load the trained model\nmodel = torchvision.models.alexnet(pretrained=True)\nnum_features = model.classifier[6].in_features\nmodel.classifier[6] = torch.nn.Linear(num_features, 5)\n\n# Load the saved state dictionary\nstate_dict = torch.load('alexnet_pretrained_5_class_fold_1.pth')\n\n# Remove the 'module.' prefix if present (for loading from a DataParallel model)\nif 'module.' in list(state_dict.keys())[0]:\n    state_dict = {k[7:]: v for k, v in state_dict.items()}\n\nmodel.load_state_dict(state_dict)\n\nmodel.eval()\n\n# Define transforms for preprocessing the image\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load and preprocess the image\nimage_path = 'C:\\\\Users\\\\suvid\\\\Desktop\\\\oyster2.jpg'\nimage = Image.open(image_path)\nimage = image.convert(\"RGB\")\n\nimage_tensor = transform(image)\nimage_tensor = image_tensor.unsqueeze(0)\n\n# Perform prediction\nwith torch.no_grad():\n    outputs = model(image_tensor)\n    _, predicted = torch.max(outputs, 1)\n    predicted_class = predicted.item()\n\n# Get class label\nclass_labels = [\"Cat\", \"Cow\", \"Octopus\", \"Oyster\",\"Panda\"]\npredicted_label = class_labels[predicted_class]\n\n# Visualize the image and prediction\nplt.imshow(image)\nplt.title(f'Predicted Class: {predicted_label}')\nplt.axis('off')\nplt.show()\n\n```\n\n\n    \n![](output_14_0.png)\n    \n\n\n# Custom Model Development\n\n## Binary Classification\n\n\n```python\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, Subset, random_split, SubsetRandomSampler, ConcatDataset\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import KFold\n\ntorch.manual_seed(1234)\ntorch.cuda.manual_seed(1234)\nnp.random.seed(1234)\n```\n\n### Data Augmentation\n\n\n```python\n# Tried to expand the dataset using data augmentation in order to improve accuracy as the dataset given was small\n\nbatch_size = 5\nnum_epochs = 15\nlearning_rate=0.001\n\nprint(torch.cuda.is_available())\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Augmentation transforms\naug_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    transforms.ToPILImage(),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.ToTensor(),\n])\n\n# Normal transforms\nstd_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ndataset1 = datasets.ImageFolder('one-vs-rest/', transform=std_transform)\ndataset2 = datasets.ImageFolder('one-vs-rest/', transform=aug_transform)\ndataset = ConcatDataset([dataset1, dataset2])\ntest_dataset = dataset\n```\n\n    True\n    \n\n\n```python\n# show random images after loading the data\n\ntrain_loader = DataLoader(dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n\ndef imshow(img):\n    # Unnormalize the image tensor\n    img = img * 0.229 + 0.485  # Undo normalization\n    npimg = img.numpy()\n    npimg = np.clip(npimg, 0, 1)  # clip values to stay within valid range\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n# get some random training images\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\nprint(labels)\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n```\n\n    tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1])\n    \n\n\n    \n![](output_20_1.png)\n    \n\n\n\n```python\n# Our Model for training\n\n# Experimented with batch normalization layers to improve accuracy, there is stil room for improvement in accuracy\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=1):\n        super(ConvNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n            #nn.BatchNorm2d(64),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n            #nn.BatchNorm2d(192),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            #nn.BatchNorm1d(4096),\n            nn.ReLU(inplace=True),\n            #nn.BatchNorm1d(4096),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.BatchNorm1d(4096),\n            nn.ReLU(inplace=True),\n            \n            nn.Linear(4096, num_classes),\n            nn.Sigmoid(),\n        )\n        self.intermediate_activations = []\n\n        # Register hooks to store intermediate activations\n        for layer in self.features:\n            if isinstance(layer, nn.Conv2d):\n                layer.register_forward_hook(self.hook_fn)\n\n    def hook_fn(self, module, input, output):\n        # Store intermediate activations\n        self.intermediate_activations.append(output)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x, self.intermediate_activations\n\n\nmodel = ConvNet().to(device)\n\n```\n\n\n```python\n# Our Model for getting the output of the convolutional layers\n\n# Experimented with batch normalization layers to improve accuracy, there is stil room for improvement in accuracy\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=1):\n        super(ConvNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n            #nn.BatchNorm2d(64),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n            #nn.BatchNorm2d(192),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            #nn.BatchNorm1d(4096),\n            nn.ReLU(inplace=True),\n            #nn.BatchNorm1d(4096),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.BatchNorm1d(4096),\n            nn.ReLU(inplace=True),\n            \n            nn.Linear(4096, num_classes),\n            nn.Sigmoid(),\n        )\n        self.intermediate_activations = []\n\n        # Register hooks to store intermediate activations\n        for layer in self.features:\n            if isinstance(layer, nn.Conv2d):\n                layer.register_forward_hook(self.hook_fn)\n\n    def hook_fn(self, module, input, output):\n        # Store intermediate activations\n        self.intermediate_activations.append(output)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x, self.intermediate_activations\n\n\nmodel = ConvNet().to(device)\n\n```\n\n\n```python\n# Define K-fold cross-validation\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\nepochs = 10\naccuracies = []\n\n# Iterate through each fold\nfor fold, (train_index, test_index) in enumerate(kf.split(dataset)):\n    # Split the augmented dataset into train and test sets for this fold\n    train_dataset = torch.utils.data.Subset(dataset, train_index)\n    test_dataset = torch.utils.data.Subset(dataset, test_index)\n\n    # Create DataLoader for train and test sets\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n    \n    model = ConvNet().to(device)\n    criterion = nn.BCELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    # Train the model\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels.float().unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n\n        print(f'Fold {fold+1}, Epoch {epoch+1}, Training Loss: {running_loss / len(train_loader.dataset):.4f}')\n\n    # Test the model\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            predicted = (outputs > 0.5).float()\n            total += labels.size(0)\n            correct += (predicted == labels.float().unsqueeze(1)).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f'Fold {fold+1}, Test Accuracy: {accuracy:.2f}%')\n    accuracies.append(accuracy)\n\n# Final accuracy\nprint(\"Final accuracy:\", np.mean(accuracies))\n```\n\n    Fold 1, Epoch 1, Training Loss: 1.8056\n    Fold 1, Epoch 2, Training Loss: 0.8210\n    Fold 1, Epoch 3, Training Loss: 0.7612\n    Fold 1, Epoch 4, Training Loss: 0.7211\n    Fold 1, Epoch 5, Training Loss: 0.6779\n    Fold 1, Epoch 6, Training Loss: 0.6805\n    Fold 1, Epoch 7, Training Loss: 0.6489\n    Fold 1, Epoch 8, Training Loss: 0.7281\n    Fold 1, Epoch 9, Training Loss: 0.6556\n    Fold 1, Epoch 10, Training Loss: 0.7384\n    Fold 1, Test Accuracy: 66.00%\n    Fold 2, Epoch 1, Training Loss: 1.3068\n    Fold 2, Epoch 2, Training Loss: 0.8174\n    Fold 2, Epoch 3, Training Loss: 0.9282\n    Fold 2, Epoch 4, Training Loss: 0.7603\n    Fold 2, Epoch 5, Training Loss: 0.7085\n    Fold 2, Epoch 6, Training Loss: 0.7218\n    Fold 2, Epoch 7, Training Loss: 0.7205\n    Fold 2, Epoch 8, Training Loss: 0.7907\n    Fold 2, Epoch 9, Training Loss: 0.6886\n    Fold 2, Epoch 10, Training Loss: 0.6475\n    Fold 2, Test Accuracy: 63.64%\n    Fold 3, Epoch 1, Training Loss: 2.1440\n    Fold 3, Epoch 2, Training Loss: 0.8926\n    Fold 3, Epoch 3, Training Loss: 0.8780\n    Fold 3, Epoch 4, Training Loss: 0.6678\n    Fold 3, Epoch 5, Training Loss: 0.8364\n    Fold 3, Epoch 6, Training Loss: 0.7241\n    Fold 3, Epoch 7, Training Loss: 0.6923\n    Fold 3, Epoch 8, Training Loss: 0.7033\n    Fold 3, Epoch 9, Training Loss: 0.7691\n    Fold 3, Epoch 10, Training Loss: 0.6823\n    Fold 3, Test Accuracy: 66.67%\n    Final accuracy: 65.43434343434343\n    \n\n\n```python\n# run this to save the model with best accuracy on the disk\n\ntorch.save(model.state_dict(), 'custom_cnn_binary.pth')\n```\n\n\n```python\nimport torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load your model\nmodel = ConvNet()\n\n# Load the model state dictionary\nmodel_path = 'custom_cnn_binary.pth'\nmodel.load_state_dict(torch.load(model_path))\n\nmodel.eval()\n\n# Initialize lists to store true labels and predicted labels\ntrue_labels = []\npred_labels = []\n\n# Iterate over the test dataset and make predictions\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        outputs = model(inputs)\n        predicted = (outputs > 0.5).float()\n        true_labels.extend(labels.cpu().numpy())\n        pred_labels.extend(predicted.cpu().numpy().flatten())\n\n# Calculate the confusion matrix\nconf_matrix = confusion_matrix(true_labels, pred_labels)\naccuracy = (conf_matrix.diagonal().sum() / conf_matrix.sum()) * 100\n\n# Plot the classification matrix using seaborn heatmap\nplt.figure(figsize=(6, 4))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cbar=True)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title(f'Classification Matrix\\nAccuracy: {accuracy:.2f}%')\nplt.show()\n\n```\n\n\n    \n![](output_25_0.png)\n    \n\n\n\n```python\nimport torch\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_grid(images, titles, prediction, actual):\n    num_images = len(images)\n    cols = min(5, num_images)\n    rows = (num_images - 1) // cols + 1\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 4))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes.flatten()[i]\n        ax.imshow(image, cmap='viridis')\n        ax.set_title(title)\n        ax.axis('off')\n    plt.suptitle(f'Prediction: {prediction}\\nActual: {actual}', fontsize=16)\n    plt.show()\n\n# Load the model from disk\nmodel_path = 'custom_cnn_binary.pth'\nmodel = ConvNet()\n\nstate_dict = torch.load(model_path)\nmodel.load_state_dict(state_dict)\n\nlabels = ['Bat', 'Others']\n\n# Get input_image tensor from test dataset\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\ndata_list = list(test_loader)\n\n# Shuffle the data list to ensure randomness\nrandom.shuffle(data_list)\n\n# Iterate through the data list until a same prediction as the actual label is obtained\nfor input_image, input_label in data_list:\n    actual = labels[input_label.cpu()]\n    \n    model.eval()\n    output, intermediate_activations = model(input_image)\n    _, predicted = torch.max(output, 1)\n    prediction = labels[predicted.cpu()]\n    \n    if prediction == actual:\n        activation_images = [activation.squeeze().detach().cpu().numpy() for activation in intermediate_activations]\n        activation_images_resized = [np.mean(image, axis=0) for image in activation_images]\n        titles = [f'Conv Layer {i+1} Activation' for i in range(len(activation_images_resized))]\n\n        # Plot intermediate activations in a grid\n        plot_grid(activation_images_resized, titles, prediction, actual)\n        break\n\n```\n\n\n    \n![](output_26_0.png)\n    \n\n\n\n```python\nimport torch\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_grid(images, titles, prediction, actual):\n    num_images = len(images)\n    cols = min(5, num_images)\n    rows = (num_images - 1) // cols + 1\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 4))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes.flatten()[i]\n        ax.imshow(image, cmap='viridis')\n        ax.set_title(title)\n        ax.axis('off')\n    plt.suptitle(f'Prediction: {prediction}\\nActual: {actual}', fontsize=16)\n    plt.show()\n\n# Load the model from disk\nmodel_path = 'custom_cnn_binary.pth'\nmodel = ConvNet()\n\nstate_dict = torch.load(model_path)\nmodel.load_state_dict(state_dict)\n\nlabels = ['Bat', 'Others']\n\n# Get input_image tensor from test dataset\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\ndata_list = list(test_loader)\n\n# Shuffle the data list to ensure randomness\nrandom.shuffle(data_list)\n\n# Iterate through the data list until a same prediction as the actual label is obtained\nfor input_image, input_label in data_list:\n    actual = labels[input_label.cpu()]\n    \n    model.eval()\n    output, intermediate_activations = model(input_image)\n    _, predicted = torch.max(output, 1)\n    prediction = labels[predicted.cpu()]\n    \n    if prediction == actual:\n        activation_images = [activation.squeeze().detach().cpu().numpy() for activation in intermediate_activations]\n        activation_images_resized = [np.mean(image, axis=0) for image in activation_images]\n        titles = [f'Conv Layer {i+1} Activation' for i in range(len(activation_images_resized))]\n\n        # Plot intermediate activations in a grid\n        plot_grid(activation_images_resized, titles, prediction, actual)\n        break\n\n```\n\n\n    \n![](output_27_0.png)\n    \n\n\n\n```python\nimport torch\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_grid(images, titles, prediction, actual):\n    num_images = len(images)\n    cols = min(5, num_images)\n    rows = (num_images - 1) // cols + 1\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 4))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes.flatten()[i]\n        ax.imshow(image, cmap='viridis')\n        ax.set_title(title)\n        ax.axis('off')\n    plt.suptitle(f'Prediction: {prediction}\\nActual: {actual}', fontsize=16)\n    plt.show()\n\n# Load the model from disk\nmodel_path = 'custom_cnn_binary.pth'\nmodel = ConvNet()\n\nstate_dict = torch.load(model_path)\nmodel.load_state_dict(state_dict)\n\nlabels = ['Bat', 'Others']\n\n# Get input_image tensor from test dataset\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\ndata_list = list(test_loader)\n\n# Shuffle the data list to ensure randomness\nrandom.shuffle(data_list)\n\n# Iterate through the data list until a same prediction as the actual label is obtained\nfor input_image, input_label in data_list:\n    actual = labels[input_label.cpu()]\n    \n    model.eval()\n    output, intermediate_activations = model(input_image)\n    _, predicted = torch.max(output, 1)\n    prediction = labels[predicted.cpu()]\n    \n    if prediction == actual:\n        activation_images = [activation.squeeze().detach().cpu().numpy() for activation in intermediate_activations]\n        activation_images_resized = [np.mean(image, axis=0) for image in activation_images]\n        titles = [f'Conv Layer {i+1} Activation' for i in range(len(activation_images_resized))]\n\n        # Plot intermediate activations in a grid\n        plot_grid(activation_images_resized, titles, prediction, actual)\n        break\n\n```\n\n\n    <Figure size 640x480 with 0 Axes>\n\n\n\n    \n![](output_28_1.png)\n    \n\n\n### Discussing automatically created features for identification of Bat\n- The model identifies the head like structure between two triangular shaped wings\n- Head or lower body both can work provided they are between two triangular shaped wings\n- It then focusses more on the head and then on the wings\n- Eventually, as we move on from one layer to another, the focus on wings also decrease, but the focus on the head increases\n\n## Binary Classification Using AlexNet architecture (Not pre-trained)\n\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom sklearn.model_selection import KFold\nfrom torchvision.models import alexnet\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Define the AlexNet architecture\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes=2):\n        super(AlexNet, self).__init__()\n        self.features = alexnet().features\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Define transformations and create dataset\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndataset = datasets.ImageFolder('one-vs-rest/', transform=transform)\n\n# Split dataset into three folds using KFold\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\n\nall_labels = []\nall_predictions = []\n\n# Step 4: Train and evaluate the model for each fold\nfor fold, (train_index, test_index) in enumerate(kf.split(dataset)):\n    train_sampler = SubsetRandomSampler(train_index)\n    test_sampler = SubsetRandomSampler(test_index)\n    train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler)\n    test_loader = DataLoader(dataset, batch_size=32, sampler=test_sampler)\n\n    # Define model, loss function, and optimizer\n    model = AlexNet(num_classes=2)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Train the model\n    for epoch in range(num_epochs):\n        model.train()\n        for inputs, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n    # Evaluate the model on test data\n    model.eval()\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            all_labels.extend(labels.tolist())\n            all_predictions.extend(predicted.tolist())\n```\n\n\n```python\n# Compute overall classification matrix\ncm = classification_matrix(all_labels, all_predictions)\n\naccuracy = (conf_matrix.diagonal().sum() / conf_matrix.sum()) * 100\n\n# Plot the classification matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d')\nplt.title(f'Classification Matrix\\nAccuracy: {accuracy:.2f}%')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()\n```\n\n\n    \n![](output_32_0.png)\n    \n\n\n## 5-Class Classification using Custom CNN\n\n\n```python\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, Subset, random_split, SubsetRandomSampler, ConcatDataset\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import KFold\n\ntorch.manual_seed(1234)\ntorch.cuda.manual_seed(1234)\nnp.random.seed(1234)\n```\n\n### Data Augmentation\n\n\n```python\n# Tried to expand the dataset using data augmentation in order to improve accuracy as the dataset given was small\n\nbatch_size = 5\nnum_epochs = 15\nlearning_rate=0.001\n\nprint(torch.cuda.is_available())\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Augmentation transforms\naug_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    transforms.ToPILImage(),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.ToTensor(),\n])\n\n# Normal transforms\nstd_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ndataset1 = datasets.ImageFolder('five-class/', transform=std_transform)\ndataset2 = datasets.ImageFolder('five-class/', transform=aug_transform)\ndataset = ConcatDataset([dataset1, dataset2])\ntest_dataset = dataset\n```\n\n    True\n    \n\n\n```python\n# show random images after loading the data\n\ntrain_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n\ndef imshow(img):\n    # Unnormalize the image tensor\n    img = img * 0.229 + 0.485\n    npimg = img.numpy()\n    npimg = np.clip(npimg, 0, 1)\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n# get some random training images\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\nprint(labels)\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n```\n\n    tensor([4, 2, 3, 3, 4, 4, 3, 4, 4, 0, 3, 0, 1, 4, 2, 4])\n    \n\n\n    \n![](output_37_1.png)\n    \n\n\n\n```python\n# Our Model for training\n\n# Experimented with batch normalization layers to improve accuracy, there is stil room for improvement in accuracy\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=5):\n        super(ConvNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n            #nn.BatchNorm2d(64),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n            #nn.BatchNorm2d(192),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            #nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.BatchNorm1d(4096),\n            nn.ReLU(inplace=True),\n            #nn.BatchNorm1d(4096),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.BatchNorm1d(4096),\n            nn.ReLU(inplace=True),\n            \n            nn.Linear(4096, num_classes),\n        )\n        \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\nmodel = ConvNet().to(device)\n\n\n```\n\n\n```python\n# Our Model for getting the output of the convolutional layers\n\n# Experimented with batch normalization layers to improve accuracy, there is stil room for improvement in accuracy\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=5):\n        super(ConvNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n            #nn.BatchNorm2d(64),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n            #nn.BatchNorm2d(192),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            #nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.BatchNorm1d(4096),\n            nn.ReLU(inplace=True),\n            #nn.BatchNorm1d(4096),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.BatchNorm1d(4096),\n            nn.ReLU(inplace=True),\n            \n            nn.Linear(4096, num_classes),\n        )\n        \n        self.intermediate_activations = []\n\n        # Register hooks to store intermediate activations\n        for layer in self.features:\n            if isinstance(layer, nn.Conv2d):\n                layer.register_forward_hook(self.hook_fn)\n\n    def hook_fn(self, module, input, output):\n        # Store intermediate activations\n        self.intermediate_activations.append(output)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x, self.intermediate_activations\n\nmodel = ConvNet().to(device)\n\n\n```\n\n\n```python\n# Define K-fold cross-validation\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\nepochs = 10\naccuracies = []\n\n# Iterate through each fold\nfor fold, (train_index, test_index) in enumerate(kf.split(dataset)):\n    # Split the augmented dataset into train and test sets for this fold\n    train_dataset = torch.utils.data.Subset(dataset, train_index)\n    test_dataset = torch.utils.data.Subset(dataset, test_index)\n\n    # Create DataLoader for train and test sets\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n    \n    # Define your model, loss function, and optimizer\n    model = ConvNet(num_classes=5).to(device)  # Assuming 5 output classes\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\n    # Train the model\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            #outputs, intermediate_activations = model(inputs)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)  \n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n\n        # Print training statistics\n        print(f'Fold {fold+1}, Epoch {epoch+1}, Training Loss: {running_loss / len(train_loader.dataset):.4f}')\n\n    # Test the model\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            #outputs, intermediate_activations = model(inputs)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)  # Get the predicted class with highest probability\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f'Fold {fold+1}, Test Accuracy: {accuracy:.2f}%')\n    accuracies.append(accuracy)\n\n# Final accuracy\nprint(\"Final accuracy:\", np.mean(accuracies))\n\n```\n\n    Fold 1, Epoch 1, Training Loss: 1.5476\n    Fold 1, Epoch 2, Training Loss: 1.2713\n    Fold 1, Epoch 3, Training Loss: 1.0316\n    Fold 1, Epoch 4, Training Loss: 0.9983\n    Fold 1, Epoch 5, Training Loss: 0.9220\n    Fold 1, Epoch 6, Training Loss: 0.8885\n    Fold 1, Epoch 7, Training Loss: 0.8359\n    Fold 1, Epoch 8, Training Loss: 0.7286\n    Fold 1, Epoch 9, Training Loss: 0.8208\n    Fold 1, Epoch 10, Training Loss: 0.7024\n    Fold 1, Test Accuracy: 56.00%\n    Fold 2, Epoch 1, Training Loss: 1.6129\n    Fold 2, Epoch 2, Training Loss: 1.1900\n    Fold 2, Epoch 3, Training Loss: 1.0651\n    Fold 2, Epoch 4, Training Loss: 0.9705\n    Fold 2, Epoch 5, Training Loss: 0.9372\n    Fold 2, Epoch 6, Training Loss: 0.8511\n    Fold 2, Epoch 7, Training Loss: 0.7492\n    Fold 2, Epoch 8, Training Loss: 0.7628\n    Fold 2, Epoch 9, Training Loss: 0.7357\n    Fold 2, Epoch 10, Training Loss: 0.6575\n    Fold 2, Test Accuracy: 59.00%\n    Fold 3, Epoch 1, Training Loss: 1.5582\n    Fold 3, Epoch 2, Training Loss: 1.1821\n    Fold 3, Epoch 3, Training Loss: 1.0665\n    Fold 3, Epoch 4, Training Loss: 1.0389\n    Fold 3, Epoch 5, Training Loss: 0.8886\n    Fold 3, Epoch 6, Training Loss: 0.8919\n    Fold 3, Epoch 7, Training Loss: 0.8593\n    Fold 3, Epoch 8, Training Loss: 0.8592\n    Fold 3, Epoch 9, Training Loss: 0.8539\n    Fold 3, Epoch 10, Training Loss: 0.7271\n    Fold 3, Test Accuracy: 59.50%\n    Final accuracy: 58.166666666666664\n    \n\n\n```python\n# run this to save the model with best accuracy on the disk\ntorch.save(model.state_dict(), 'custom_cnn_5_class.pth')\n```\n\n\n```python\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nnum_classes = 5\n\n# Initialize the model\nmodel = ConvNet(num_classes).to(device)\nmodel.load_state_dict(torch.load('custom_cnn_5_class.pth'))\nmodel.eval()\n\ntrue_labels = []\npred_labels = []\n\n# Iterate over the test dataset and make predictions\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        true_labels.extend(labels.cpu().numpy())\n        pred_labels.extend(predicted.cpu().numpy())\n\n# Calculate the classification matrix\nconf_matrix = confusion_matrix(true_labels, pred_labels)\n\n# Calculate accuracy using the classification matrix\naccuracy = (np.diag(conf_matrix).sum() / conf_matrix.sum()) * 100\n\n# Plot the classification matrix using seaborn heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True,fmt='d', cbar=True)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title(f'Classification Matrix\\nAccuracy: {accuracy:.2f}%')\nplt.show()\n\n```\n\n\n    \n![](output_42_0.png)\n    \n\n\n## Visualizing Output of CNN Layers and looking for Automatically created Features\n\n\n```python\nimport torch\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_grid(images, titles, rows, cols, prediction, actual):\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 4))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes.flatten()[i]\n        ax.imshow(image, cmap='viridis')\n        ax.set_title(title)\n        ax.axis('off')\n    plt.suptitle(f'Prediction: {prediction}\\nActual: {actual}',fontsize=16)\n    plt.show()\n\n# Load the model from disk\nmodel_path = 'custom_cnn_5_class.pth'\nmodel = ConvNet()\n\n# Load the state dictionary of the model\nstate_dict = torch.load(model_path)\n\n# Load the state dictionary into the model\nmodel.load_state_dict(state_dict)\nlabels=['Cat','Cow','Octopus','Oyster','Panda']\n# Get input_image tensor from test dataset\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\ndata_list = list(test_loader)\n\n# Get a random item from the list\nrandom_item = random.choice(data_list)\n\n# Extract image and label from the random item\ninput_image, input_label = random_item\nactual = labels[input_label.cpu()]\n\nmodel.eval()\noutput, intermediate_activations = model(input_image)\n_, predicted = torch.max(output, 1)\nprediction = labels[predicted.cpu()]\nwhile prediction != actual:\n    # Get a random item from the list\n    random_item = random.choice(data_list)\n    \n    # Extract image and label from the random item\n    input_image, input_label = random_item\n    actual = labels[input_label.cpu()]\n    \n    output, intermediate_activations = model(input_image)\n    _, predicted = torch.max(output, 1)\n    prediction = labels[predicted.cpu()]\n    \n# Resize activation maps to a square shape for plotting\nactivation_images = [activation.squeeze().detach().cpu().numpy() for activation in intermediate_activations]\nactivation_images_resized = [np.mean(image, axis=0) for image in activation_images]  # Compute channel-wise mean\ntitles = [f'Conv Layer {i+1} Activation' for i in range(len(activation_images_resized))]\n\n# Plot intermediate activations in a grid\nplot_grid(activation_images_resized, titles, 1, 5, prediction, actual)\n```\n\n\n    \n![](output_44_0.png)\n    \n\n\n\n```python\nimport torch\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_grid(images, titles, rows, cols, prediction, actual):\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 4))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes.flatten()[i]\n        ax.imshow(image, cmap='viridis')\n        ax.set_title(title)\n        ax.axis('off')\n    plt.suptitle(f'Prediction: {prediction}\\nActual: {actual}',fontsize=16)\n    plt.show()\n\n# Load the model from disk\nmodel_path = 'custom_cnn_5_class.pth'\nmodel = ConvNet()\n\n# Load the state dictionary of the model\nstate_dict = torch.load(model_path)\n\n# Load the state dictionary into the model\nmodel.load_state_dict(state_dict)\nlabels=['Cat','Cow','Octopus','Oyster','Panda']\n# Get input_image tensor from test dataset\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\ndata_list = list(test_loader)\n\n# Get a random item from the list\nrandom_item = random.choice(data_list)\n\n# Extract image and label from the random item\ninput_image, input_label = random_item\nactual = labels[input_label.cpu()]\n\nmodel.eval()\noutput, intermediate_activations = model(input_image)\n_, predicted = torch.max(output, 1)\nprediction = labels[predicted.cpu()]\nwhile prediction != actual:\n    # Get a random item from the list\n    random_item = random.choice(data_list)\n    \n    # Extract image and label from the random item\n    input_image, input_label = random_item\n    actual = labels[input_label.cpu()]\n    \n    output, intermediate_activations = model(input_image)\n    _, predicted = torch.max(output, 1)\n    prediction = labels[predicted.cpu()]\n    \n# Resize activation maps to a square shape for plotting\nactivation_images = [activation.squeeze().detach().cpu().numpy() for activation in intermediate_activations]\nactivation_images_resized = [np.mean(image, axis=0) for image in activation_images]  # Compute channel-wise mean\ntitles = [f'Conv Layer {i+1} Activation' for i in range(len(activation_images_resized))]\n\n# Plot intermediate activations in a grid\nplot_grid(activation_images_resized, titles, 1, 5, prediction, actual)\n```\n\n\n    \n![](output_45_0.png)\n    \n\n\n### Discussing automatically created features for identification of Panda\n- The model identifies the head and body\n- It identified facial features in the first layer such as eyes shaped as panda, nose, mputh and ears\n- As we proceed in layers, the focus on head and body increases and facial features are disappeared\n\n\n```python\nimport torch\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_grid(images, titles, rows, cols, prediction, actual):\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 4))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes.flatten()[i]\n        ax.imshow(image, cmap='viridis')\n        ax.set_title(title)\n        ax.axis('off')\n    plt.suptitle(f'Prediction: {prediction}\\nActual: {actual}',fontsize=16)\n    plt.show()\n\n# Load the model from disk\nmodel_path = 'custom_cnn_5_class.pth'\nmodel = ConvNet()\n\n# Load the state dictionary of the model\nstate_dict = torch.load(model_path)\n\n# Load the state dictionary into the model\nmodel.load_state_dict(state_dict)\nlabels=['Cat','Cow','Octopus','Oyster','Panda']\n# Get input_image tensor from test dataset\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\ndata_list = list(test_loader)\n\n# Get a random item from the list\nrandom_item = random.choice(data_list)\n\n# Extract image and label from the random item\ninput_image, input_label = random_item\nactual = labels[input_label.cpu()]\n\nmodel.eval()\noutput, intermediate_activations = model(input_image)\n_, predicted = torch.max(output, 1)\nprediction = labels[predicted.cpu()]\nwhile prediction != actual:\n    # Get a random item from the list\n    random_item = random.choice(data_list)\n    \n    # Extract image and label from the random item\n    input_image, input_label = random_item\n    actual = labels[input_label.cpu()]\n    \n    output, intermediate_activations = model(input_image)\n    _, predicted = torch.max(output, 1)\n    prediction = labels[predicted.cpu()]\n    \n# Resize activation maps to a square shape for plotting\nactivation_images = [activation.squeeze().detach().cpu().numpy() for activation in intermediate_activations]\nactivation_images_resized = [np.mean(image, axis=0) for image in activation_images]  # Compute channel-wise mean\ntitles = [f'Conv Layer {i+1} Activation' for i in range(len(activation_images_resized))]\n\n# Plot intermediate activations in a grid\nplot_grid(activation_images_resized, titles, 1, 5, prediction, actual)\n```\n\n\n    \n![](output_47_0.png)\n    \n\n\n### Discussing automatically created features for identification of Octopus\n- The model identifies the head and tentacles\n- The focus in more on head like structure between or near tentacles connected to the it\n- As we proceed in layers, the focus on head also reduces a bit\n\n\n```python\nimport torch\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_grid(images, titles, rows, cols, prediction, actual):\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 4))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes.flatten()[i]\n        ax.imshow(image, cmap='viridis')\n        ax.set_title(title)\n        ax.axis('off')\n    plt.suptitle(f'Prediction: {prediction}\\nActual: {actual}',fontsize=16)\n    plt.show()\n\n# Load the model from disk\nmodel_path = 'custom_cnn_5_class.pth'\nmodel = ConvNet()\n\n# Load the state dictionary of the model\nstate_dict = torch.load(model_path)\n\n# Load the state dictionary into the model\nmodel.load_state_dict(state_dict)\nlabels=['Cat','Cow','Octopus','Oyster','Panda']\n# Get input_image tensor from test dataset\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\ndata_list = list(test_loader)\n\n# Get a random item from the list\nrandom_item = random.choice(data_list)\n\n# Extract image and label from the random item\ninput_image, input_label = random_item\nactual = labels[input_label.cpu()]\n\nmodel.eval()\noutput, intermediate_activations = model(input_image)\n_, predicted = torch.max(output, 1)\nprediction = labels[predicted.cpu()]\nwhile prediction != actual:\n    # Get a random item from the list\n    random_item = random.choice(data_list)\n    \n    # Extract image and label from the random item\n    input_image, input_label = random_item\n    actual = labels[input_label.cpu()]\n    \n    output, intermediate_activations = model(input_image)\n    _, predicted = torch.max(output, 1)\n    prediction = labels[predicted.cpu()]\n    \n# Resize activation maps to a square shape for plotting\nactivation_images = [activation.squeeze().detach().cpu().numpy() for activation in intermediate_activations]\nactivation_images_resized = [np.mean(image, axis=0) for image in activation_images]  # Compute channel-wise mean\ntitles = [f'Conv Layer {i+1} Activation' for i in range(len(activation_images_resized))]\n\n# Plot intermediate activations in a grid\nplot_grid(activation_images_resized, titles, 1, 5, prediction, actual)\n```\n\n\n    \n![](output_49_0.png)\n    \n\n\n### Discussing automatically created features for identification of Oyster\n- The model identifies the spaces in between and looks for oyster-like shape\n- It looks for slot like structures\n- It then focusses on the actual oyster more, eliminating the background\n\n\n```python\nimport torch\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_grid(images, titles, rows, cols, prediction, actual):\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 4))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes.flatten()[i]\n        ax.imshow(image, cmap='viridis')\n        ax.set_title(title)\n        ax.axis('off')\n    plt.suptitle(f'Prediction: {prediction}\\nActual: {actual}',fontsize=16)\n    plt.show()\n\n# Load the model from disk\nmodel_path = 'custom_cnn_5_class.pth'\nmodel = ConvNet()\n\n# Load the state dictionary of the model\nstate_dict = torch.load(model_path)\n\n# Load the state dictionary into the model\nmodel.load_state_dict(state_dict)\nlabels=['Cat','Cow','Octopus','Oyster','Panda']\n# Get input_image tensor from test dataset\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\ndata_list = list(test_loader)\n\n# Get a random item from the list\nrandom_item = random.choice(data_list)\n\n# Extract image and label from the random item\ninput_image, input_label = random_item\nactual = labels[input_label.cpu()]\n\nmodel.eval()\noutput, intermediate_activations = model(input_image)\n_, predicted = torch.max(output, 1)\nprediction = labels[predicted.cpu()]\nwhile prediction != actual:\n    # Get a random item from the list\n    random_item = random.choice(data_list)\n    \n    # Extract image and label from the random item\n    input_image, input_label = random_item\n    actual = labels[input_label.cpu()]\n    \n    output, intermediate_activations = model(input_image)\n    _, predicted = torch.max(output, 1)\n    prediction = labels[predicted.cpu()]\n    \n# Resize activation maps to a square shape for plotting\nactivation_images = [activation.squeeze().detach().cpu().numpy() for activation in intermediate_activations]\nactivation_images_resized = [np.mean(image, axis=0) for image in activation_images]  # Compute channel-wise mean\ntitles = [f'Conv Layer {i+1} Activation' for i in range(len(activation_images_resized))]\n\n# Plot intermediate activations in a grid\nplot_grid(activation_images_resized, titles, 1, 5, prediction, actual)\n```\n\n\n    \n![](output_51_0.png)\n    \n\n\n### Discussing automatically created features for identification of Cat\n- The model identifies the and the eyes, nose etc. (facial features) first\n- It then focusses more on the head and jaw\n- Eventually, as we move on from one layer to another, the focus on the head increases\n\n## Binary Classification using AlexNet (not pre-trained)\n\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom sklearn.model_selection import KFold\nfrom torchvision.models import alexnet\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_epochs = 10\n\n# Define the AlexNet architecture\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes=5):\n        super(AlexNet, self).__init__()\n        self.features = alexnet().features\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Define transformations and create dataset\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Update dataset directory to your 5-class dataset directory\ndataset = datasets.ImageFolder('five-class/', transform=transform)\n\n# Split dataset into three folds using KFold\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\n\nall_labels = []\nall_predictions = []\nall_accuracies = []\n\n# Train and evaluate the model for each fold\nfor fold, (train_index, test_index) in enumerate(kf.split(dataset)):\n    train_sampler = SubsetRandomSampler(train_index)\n    test_sampler = SubsetRandomSampler(test_index)\n    train_loader = DataLoader(dataset, batch_size=16, sampler=train_sampler)\n    test_loader = DataLoader(dataset, batch_size=16, sampler=test_sampler)\n\n    # Define model, loss function, and optimizer\n    model = AlexNet().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n    # Training loop\n    for epoch in range(num_epochs):  # You need to define num_epochs\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n        print(f'Fold {fold+1}, Epoch {epoch+1}, Training Loss: {running_loss / len(train_loader.dataset):.4f}')\n\n    # Evaluate the model on test data after training\n    model.eval()\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            all_labels.extend(labels.tolist())\n            all_predictions.extend(predicted.tolist())\n    \n    # Calculate accuracy for the current fold\n    accuracy = accuracy_score(all_labels, all_predictions)\n    all_accuracies.append(accuracy)\n    print(f'Fold [{fold + 1}] Accuracy: {accuracy:.4f}')\n\n# Calculate final accuracy across all folds\nfinal_accuracy = np.mean(all_accuracies)\nprint(f'Final Accuracy: {final_accuracy:.4f}')\n```\n\n    Fold 1, Epoch 1, Training Loss: 1.0693\n    Fold 1, Epoch 2, Training Loss: 1.0204\n    Fold 1, Epoch 3, Training Loss: 0.9608\n    Fold 1, Epoch 4, Training Loss: 0.8554\n    Fold 1, Epoch 5, Training Loss: 0.9104\n    Fold 1, Epoch 6, Training Loss: 0.8728\n    Fold 1, Epoch 7, Training Loss: 0.7820\n    Fold 1, Epoch 8, Training Loss: 0.7446\n    Fold 1, Epoch 9, Training Loss: 0.6400\n    Fold 1, Epoch 10, Training Loss: 0.6361\n    Fold [1] Accuracy: 0.3800\n    Fold 2, Epoch 1, Training Loss: 1.0755\n    Fold 2, Epoch 2, Training Loss: 1.0533\n    Fold 2, Epoch 3, Training Loss: 1.0308\n    Fold 2, Epoch 4, Training Loss: 0.9636\n    Fold 2, Epoch 5, Training Loss: 0.8872\n    Fold 2, Epoch 6, Training Loss: 0.8367\n    Fold 2, Epoch 7, Training Loss: 0.8187\n    Fold 2, Epoch 8, Training Loss: 0.9069\n    Fold 2, Epoch 9, Training Loss: 0.7060\n    Fold 2, Epoch 10, Training Loss: 0.6765\n    Fold [2] Accuracy: 0.4600\n    Fold 3, Epoch 1, Training Loss: 1.0750\n    Fold 3, Epoch 2, Training Loss: 1.0664\n    Fold 3, Epoch 3, Training Loss: 1.0445\n    Fold 3, Epoch 4, Training Loss: 0.9687\n    Fold 3, Epoch 5, Training Loss: 0.9161\n    Fold 3, Epoch 6, Training Loss: 0.8616\n    Fold 3, Epoch 7, Training Loss: 0.8181\n    Fold 3, Epoch 8, Training Loss: 0.8172\n    Fold 3, Epoch 9, Training Loss: 0.7912\n    Fold 3, Epoch 10, Training Loss: 0.6597\n    Fold [3] Accuracy: 0.4967\n    Final Accuracy: 0.4456\n    \n\n\n```python\n# Calculate and plot confusion matrix\nconf_matrix = confusion_matrix(all_labels, all_predictions)\n\n# Calculate accuracy using the classification matrix\naccuracy = (np.diag(conf_matrix).sum() / conf_matrix.sum()) * 100\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cbar=True)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title(f'Classification Matrix\\nAccuracy: {accuracy:.2f}%')\nplt.show()\n```\n\n\n    \n![](output_55_0.png)\n    \n\n\n# Thank You!\n","srcMarkdownNoYaml":"\n\n# Selection Task\n\nI used an NVIDIA RTX 3050 for this task but the code will work on CPU as well\n\nFulfilled all the requirements by implementing the following:\n- Organized the dataset for one-vs-rest (binary) classification\n- Organized the dataset for 5-class classification\n- Performed binary classification using existing architecture (pre-trained AlexNet)\n- Performed 5-class classification using existing architecture (pre-trained AlexNet)\n- Performed binary classification using existing architecture (AlexNet trained from scratch on the given dataset)\n- Performed 5-class classification using existing architecture (AlexNet trained from scratch on the given dataset)\n- Created a custom model for binary classification and trained it on the given dataset\n- Created a custom model for 5-class classification and trained it on the given dataset\n- Used 3-fold validation for calculating the accuracies of the models\n- Generated classification matrices for the models and also calculated accuracy using it\n- Visualized the output of all the convolutional layers for different classes in the custom CNN model\n- Discussed my insights on the automatically created features by the custom CNN\n\nI was not able to upload pre-trained models on GitHub due to size limit\n\n### Download the entire code and my trained models here:\n\n### [Click Here](https://drive.google.com/drive/folders/1Pb5a-NpP4SW-E9FfP1hIZjBD4k_Gz95t?usp=drive_link)\n\n\nThe directory structure should look like this:\n![directory_structure.png](directory_structure.png)\n\n## Restructuring for One-vs-Rest\n\nThe \"animals\" directory contains 90 different labels and each folder contains 60 images of each animal\n\nFor One-vs-Rest classification, we'll choose bat as our positive sample\n\nThe following code creates the appropriate directory structure, if it does not exist.\n\nThis notebook must be present in the same directory as the \"animal\" directory for the code to work.\n\nSelecting 5 random images from each class to build the \"others\" label.\n\n\n```python\nimport os\nimport shutil\nimport random\nsrc_folder = \"animals/bat\"\nfolders = os.listdir(\"animals\")\ntemp = []\nfor folder in folders:\n    if folder != \"bat\":\n        temp.append([os.path.join(\"animals/\"+folder, file) for file in random.sample(os.listdir(\"animals/\"+folder),1)])\nto_copy=[]\nfor temp_list in temp:\n    for item in temp_list:\n        to_copy.append(item)\nif not os.path.exists(\"one-vs-rest\"):\n    shutil.copytree(src_folder, \"one-vs-rest/bat\")\n    os.mkdir(\"one-vs-rest/others\")\n    for file_name in to_copy:\n        shutil.copy2(file_name, \"one-vs-rest/others/\")\n```\n\n## Using pre-trained existing architecture (AlexNet) for binary classification (One vs Rest)\n\n\n```python\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\nnum_epochs = 10\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load dataset\ndataset = torchvision.datasets.ImageFolder(root='one-vs-rest/', transform=transform)\n\n# Define k-fold cross-validation\nkfold = KFold(n_splits=3, shuffle=True, random_state=42)\n\naccuracies=[]\n\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n    print(f\"Fold {fold+1}\")\n\n    # Split dataset into train and validation subsets\n    train_subset = Subset(dataset, train_idx)\n    val_subset = Subset(dataset, val_idx)\n\n    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n\n    # Load pre-trained AlexNet model\n    model = torchvision.models.alexnet(pretrained=True)\n\n    # Freeze the parameters of the pre-trained layers\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Modify the last fully connected layer for binary classification\n    num_ftrs = model.classifier[6].in_features\n    model.classifier[6] = nn.Linear(num_ftrs, 2)\n\n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Train the model\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f\"Epoch {epoch+1}/{5}, Training Loss: {epoch_loss:.4f}\")\n\n    # Evaluate the model\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    accuracy = correct / total\n    accuracies.append(accuracy)\n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n\n    # Save the model\n    torch.save(model.state_dict(), f'alexnet_ovr_fold_{fold+1}.pth')\n\nprint(\"\\nFinal Mean Accuracy:\", '{0:.4f}'.format(np.mean(accuracies)))\n```\n\n    Fold 1\n    \n\n    C:\\Users\\suvid\\.conda\\envs\\intern\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n      warnings.warn(\n    C:\\Users\\suvid\\.conda\\envs\\intern\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n      warnings.warn(msg)\n    \n\n    Epoch 1/5, Training Loss: 0.4932\n    Epoch 2/5, Training Loss: 0.1409\n    Epoch 3/5, Training Loss: 0.0635\n    Epoch 4/5, Training Loss: 0.0384\n    Epoch 5/5, Training Loss: 0.0199\n    Validation Accuracy: 0.9625\n    Fold 2\n    Epoch 1/5, Training Loss: 0.5235\n    Epoch 2/5, Training Loss: 0.1369\n    Epoch 3/5, Training Loss: 0.0816\n    Epoch 4/5, Training Loss: 0.0291\n    Epoch 5/5, Training Loss: 0.0238\n    Validation Accuracy: 0.8861\n    Fold 3\n    Epoch 1/5, Training Loss: 0.6815\n    Epoch 2/5, Training Loss: 0.2231\n    Epoch 3/5, Training Loss: 0.1060\n    Epoch 4/5, Training Loss: 0.0705\n    Epoch 5/5, Training Loss: 0.0449\n    Validation Accuracy: 0.9620\n    \n    Final Mean Accuracy: 0.9369\n    \n\n\n```python\n#Predicting\n\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Load the trained model\nmodel = torchvision.models.alexnet(pretrained=True)\nnum_features = model.classifier[6].in_features\nmodel.classifier[6] = torch.nn.Linear(num_features, 2)\n\n# Load the saved state dictionary\nstate_dict = torch.load('alexnet_ovr_fold_1.pth')\n\n# Remove the 'module.' prefix if present (for loading from a DataParallel model)\nif 'module.' in list(state_dict.keys())[0]:\n    state_dict = {k[7:]: v for k, v in state_dict.items()}\n\n# Load the state dictionary into the model\nmodel.load_state_dict(state_dict)\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Define transforms for preprocessing the image\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load and preprocess the image\nimage_path = 'C:\\\\Users\\\\suvid\\\\Desktop\\\\fruitbat.jpg'  # Replace with the path to your image\nimage = Image.open(image_path)\nimage = image.convert(\"RGB\")\n\nimage_tensor = transform(image)\nimage_tensor = image_tensor.unsqueeze(0)  # Add batch dimension\n\n# Perform prediction\nwith torch.no_grad():\n    outputs = model(image_tensor)\n    _, predicted = torch.max(outputs, 1)\n    predicted_class = predicted.item()\n\n# Get class label\nclass_labels = ['Bat', 'Others']\npredicted_label = class_labels[predicted_class]\n\n# Visualize the image and prediction\nplt.imshow(image)\nplt.title(f'Predicted Class: {predicted_label}')\nplt.axis('off')\nplt.show()\n\n```\n\n\n    \n![](output_8_0.png)\n    \n\n\n## Restructuring for 5 Class Classification\n\nWe're taking 5 labels: cat, cow, octopus, oyster and panda\n\n\n```python\nimport os\nimport shutil\nimport random\n\nif not os.path.exists(\"five-class\"):\n    shutil.copytree(\"animals/cat\", \"five-class/cat\")\n    shutil.copytree(\"animals/cow\", \"five-class/cow\")\n    shutil.copytree(\"animals/octopus\", \"five-class/octopus\")\n    shutil.copytree(\"animals/oyster\", \"five-class/oyster\")\n    shutil.copytree(\"animals/panda\", \"five-class/panda\")\n\n```\n\n## 5-Class Classification using pre-trained AlexNet\n\n\n```python\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.model_selection import KFold\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load dataset\ndataset = torchvision.datasets.ImageFolder(root='five-class/', transform=transform)\n\n# Define k-fold cross-validation\nkfold = KFold(n_splits=3, shuffle=True, random_state=42)\n\naccuracies=[]\n\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n    print(f\"Fold {fold+1}\")\n\n    # Split dataset into train and validation subsets\n    train_subset = Subset(dataset, train_idx)\n    val_subset = Subset(dataset, val_idx)\n\n    # Create data loaders\n    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n\n    # Load pre-trained AlexNet model\n    model = torchvision.models.alexnet(pretrained=True)\n\n    # Modify the final fully connected layer for 5-class classification\n    num_ftrs = model.classifier[6].in_features\n    model.classifier[6] = nn.Linear(num_ftrs, 5)  # 5 output classes for 5-class classification\n\n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n\n    # Train the model\n    for epoch in range(5):  # Number of epochs\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n        epoch_loss = running_loss / len(train_loader.dataset)\n        print(f\"Epoch {epoch+1}/{5}, Training Loss: {epoch_loss:.4f}\")\n\n    # Evaluate the model\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    accuracy = correct / total\n    accuracies.append(accuracy)\n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n\n    # Save the model\n    torch.save(model.state_dict(), f'alexnet_pretrained_5_class_fold_{fold+1}.pth')\n\nprint(\"\\nFinal Mean Accuracy:\", '{0:.4f}'.format(np.mean(accuracies)))\n```\n\n    Fold 1\n    Epoch 1/5, Training Loss: 2.0340\n    Epoch 2/5, Training Loss: 1.5618\n    Epoch 3/5, Training Loss: 1.2037\n    Epoch 4/5, Training Loss: 0.9442\n    Epoch 5/5, Training Loss: 0.7466\n    Validation Accuracy: 0.8000\n    Fold 2\n    Epoch 1/5, Training Loss: 1.8057\n    Epoch 2/5, Training Loss: 1.3289\n    Epoch 3/5, Training Loss: 0.9609\n    Epoch 4/5, Training Loss: 0.7331\n    Epoch 5/5, Training Loss: 0.5253\n    Validation Accuracy: 0.8100\n    Fold 3\n    Epoch 1/5, Training Loss: 1.7709\n    Epoch 2/5, Training Loss: 1.3511\n    Epoch 3/5, Training Loss: 0.9841\n    Epoch 4/5, Training Loss: 0.7640\n    Epoch 5/5, Training Loss: 0.5536\n    Validation Accuracy: 0.7900\n    \n    Final Mean Accuracy: 0.8000\n    \n\n\n```python\n#Predicting\n\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Load the trained model\nmodel = torchvision.models.alexnet(pretrained=True)\nnum_features = model.classifier[6].in_features\nmodel.classifier[6] = torch.nn.Linear(num_features, 5)\n\n# Load the saved state dictionary\nstate_dict = torch.load('alexnet_pretrained_5_class_fold_1.pth')\n\n# Remove the 'module.' prefix if present (for loading from a DataParallel model)\nif 'module.' in list(state_dict.keys())[0]:\n    state_dict = {k[7:]: v for k, v in state_dict.items()}\n\nmodel.load_state_dict(state_dict)\n\nmodel.eval()\n\n# Define transforms for preprocessing the image\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load and preprocess the image\nimage_path = 'C:\\\\Users\\\\suvid\\\\Desktop\\\\oyster2.jpg'\nimage = Image.open(image_path)\nimage = image.convert(\"RGB\")\n\nimage_tensor = transform(image)\nimage_tensor = image_tensor.unsqueeze(0)\n\n# Perform prediction\nwith torch.no_grad():\n    outputs = model(image_tensor)\n    _, predicted = torch.max(outputs, 1)\n    predicted_class = predicted.item()\n\n# Get class label\nclass_labels = [\"Cat\", \"Cow\", \"Octopus\", \"Oyster\",\"Panda\"]\npredicted_label = class_labels[predicted_class]\n\n# Visualize the image and prediction\nplt.imshow(image)\nplt.title(f'Predicted Class: {predicted_label}')\nplt.axis('off')\nplt.show()\n\n```\n\n\n    \n![](output_14_0.png)\n    \n\n\n# Custom Model Development\n\n## Binary Classification\n\n\n```python\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, Subset, random_split, SubsetRandomSampler, ConcatDataset\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import KFold\n\ntorch.manual_seed(1234)\ntorch.cuda.manual_seed(1234)\nnp.random.seed(1234)\n```\n\n### Data Augmentation\n\n\n```python\n# Tried to expand the dataset using data augmentation in order to improve accuracy as the dataset given was small\n\nbatch_size = 5\nnum_epochs = 15\nlearning_rate=0.001\n\nprint(torch.cuda.is_available())\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Augmentation transforms\naug_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    transforms.ToPILImage(),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.ToTensor(),\n])\n\n# Normal transforms\nstd_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ndataset1 = datasets.ImageFolder('one-vs-rest/', transform=std_transform)\ndataset2 = datasets.ImageFolder('one-vs-rest/', transform=aug_transform)\ndataset = ConcatDataset([dataset1, dataset2])\ntest_dataset = dataset\n```\n\n    True\n    \n\n\n```python\n# show random images after loading the data\n\ntrain_loader = DataLoader(dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n\ndef imshow(img):\n    # Unnormalize the image tensor\n    img = img * 0.229 + 0.485  # Undo normalization\n    npimg = img.numpy()\n    npimg = np.clip(npimg, 0, 1)  # clip values to stay within valid range\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n# get some random training images\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\nprint(labels)\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n```\n\n    tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1])\n    \n\n\n    \n![](output_20_1.png)\n    \n\n\n\n```python\n# Our Model for training\n\n# Experimented with batch normalization layers to improve accuracy, there is stil room for improvement in accuracy\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=1):\n        super(ConvNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n            #nn.BatchNorm2d(64),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n            #nn.BatchNorm2d(192),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            #nn.BatchNorm1d(4096),\n            nn.ReLU(inplace=True),\n            #nn.BatchNorm1d(4096),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.BatchNorm1d(4096),\n            nn.ReLU(inplace=True),\n            \n            nn.Linear(4096, num_classes),\n            nn.Sigmoid(),\n        )\n        self.intermediate_activations = []\n\n        # Register hooks to store intermediate activations\n        for layer in self.features:\n            if isinstance(layer, nn.Conv2d):\n                layer.register_forward_hook(self.hook_fn)\n\n    def hook_fn(self, module, input, output):\n        # Store intermediate activations\n        self.intermediate_activations.append(output)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x, self.intermediate_activations\n\n\nmodel = ConvNet().to(device)\n\n```\n\n\n```python\n# Our Model for getting the output of the convolutional layers\n\n# Experimented with batch normalization layers to improve accuracy, there is stil room for improvement in accuracy\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=1):\n        super(ConvNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n            #nn.BatchNorm2d(64),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n            #nn.BatchNorm2d(192),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            #nn.BatchNorm1d(4096),\n            nn.ReLU(inplace=True),\n            #nn.BatchNorm1d(4096),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.BatchNorm1d(4096),\n            nn.ReLU(inplace=True),\n            \n            nn.Linear(4096, num_classes),\n            nn.Sigmoid(),\n        )\n        self.intermediate_activations = []\n\n        # Register hooks to store intermediate activations\n        for layer in self.features:\n            if isinstance(layer, nn.Conv2d):\n                layer.register_forward_hook(self.hook_fn)\n\n    def hook_fn(self, module, input, output):\n        # Store intermediate activations\n        self.intermediate_activations.append(output)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x, self.intermediate_activations\n\n\nmodel = ConvNet().to(device)\n\n```\n\n\n```python\n# Define K-fold cross-validation\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\nepochs = 10\naccuracies = []\n\n# Iterate through each fold\nfor fold, (train_index, test_index) in enumerate(kf.split(dataset)):\n    # Split the augmented dataset into train and test sets for this fold\n    train_dataset = torch.utils.data.Subset(dataset, train_index)\n    test_dataset = torch.utils.data.Subset(dataset, test_index)\n\n    # Create DataLoader for train and test sets\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n    \n    model = ConvNet().to(device)\n    criterion = nn.BCELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    # Train the model\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels.float().unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n\n        print(f'Fold {fold+1}, Epoch {epoch+1}, Training Loss: {running_loss / len(train_loader.dataset):.4f}')\n\n    # Test the model\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            predicted = (outputs > 0.5).float()\n            total += labels.size(0)\n            correct += (predicted == labels.float().unsqueeze(1)).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f'Fold {fold+1}, Test Accuracy: {accuracy:.2f}%')\n    accuracies.append(accuracy)\n\n# Final accuracy\nprint(\"Final accuracy:\", np.mean(accuracies))\n```\n\n    Fold 1, Epoch 1, Training Loss: 1.8056\n    Fold 1, Epoch 2, Training Loss: 0.8210\n    Fold 1, Epoch 3, Training Loss: 0.7612\n    Fold 1, Epoch 4, Training Loss: 0.7211\n    Fold 1, Epoch 5, Training Loss: 0.6779\n    Fold 1, Epoch 6, Training Loss: 0.6805\n    Fold 1, Epoch 7, Training Loss: 0.6489\n    Fold 1, Epoch 8, Training Loss: 0.7281\n    Fold 1, Epoch 9, Training Loss: 0.6556\n    Fold 1, Epoch 10, Training Loss: 0.7384\n    Fold 1, Test Accuracy: 66.00%\n    Fold 2, Epoch 1, Training Loss: 1.3068\n    Fold 2, Epoch 2, Training Loss: 0.8174\n    Fold 2, Epoch 3, Training Loss: 0.9282\n    Fold 2, Epoch 4, Training Loss: 0.7603\n    Fold 2, Epoch 5, Training Loss: 0.7085\n    Fold 2, Epoch 6, Training Loss: 0.7218\n    Fold 2, Epoch 7, Training Loss: 0.7205\n    Fold 2, Epoch 8, Training Loss: 0.7907\n    Fold 2, Epoch 9, Training Loss: 0.6886\n    Fold 2, Epoch 10, Training Loss: 0.6475\n    Fold 2, Test Accuracy: 63.64%\n    Fold 3, Epoch 1, Training Loss: 2.1440\n    Fold 3, Epoch 2, Training Loss: 0.8926\n    Fold 3, Epoch 3, Training Loss: 0.8780\n    Fold 3, Epoch 4, Training Loss: 0.6678\n    Fold 3, Epoch 5, Training Loss: 0.8364\n    Fold 3, Epoch 6, Training Loss: 0.7241\n    Fold 3, Epoch 7, Training Loss: 0.6923\n    Fold 3, Epoch 8, Training Loss: 0.7033\n    Fold 3, Epoch 9, Training Loss: 0.7691\n    Fold 3, Epoch 10, Training Loss: 0.6823\n    Fold 3, Test Accuracy: 66.67%\n    Final accuracy: 65.43434343434343\n    \n\n\n```python\n# run this to save the model with best accuracy on the disk\n\ntorch.save(model.state_dict(), 'custom_cnn_binary.pth')\n```\n\n\n```python\nimport torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load your model\nmodel = ConvNet()\n\n# Load the model state dictionary\nmodel_path = 'custom_cnn_binary.pth'\nmodel.load_state_dict(torch.load(model_path))\n\nmodel.eval()\n\n# Initialize lists to store true labels and predicted labels\ntrue_labels = []\npred_labels = []\n\n# Iterate over the test dataset and make predictions\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        outputs = model(inputs)\n        predicted = (outputs > 0.5).float()\n        true_labels.extend(labels.cpu().numpy())\n        pred_labels.extend(predicted.cpu().numpy().flatten())\n\n# Calculate the confusion matrix\nconf_matrix = confusion_matrix(true_labels, pred_labels)\naccuracy = (conf_matrix.diagonal().sum() / conf_matrix.sum()) * 100\n\n# Plot the classification matrix using seaborn heatmap\nplt.figure(figsize=(6, 4))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cbar=True)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title(f'Classification Matrix\\nAccuracy: {accuracy:.2f}%')\nplt.show()\n\n```\n\n\n    \n![](output_25_0.png)\n    \n\n\n\n```python\nimport torch\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_grid(images, titles, prediction, actual):\n    num_images = len(images)\n    cols = min(5, num_images)\n    rows = (num_images - 1) // cols + 1\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 4))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes.flatten()[i]\n        ax.imshow(image, cmap='viridis')\n        ax.set_title(title)\n        ax.axis('off')\n    plt.suptitle(f'Prediction: {prediction}\\nActual: {actual}', fontsize=16)\n    plt.show()\n\n# Load the model from disk\nmodel_path = 'custom_cnn_binary.pth'\nmodel = ConvNet()\n\nstate_dict = torch.load(model_path)\nmodel.load_state_dict(state_dict)\n\nlabels = ['Bat', 'Others']\n\n# Get input_image tensor from test dataset\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\ndata_list = list(test_loader)\n\n# Shuffle the data list to ensure randomness\nrandom.shuffle(data_list)\n\n# Iterate through the data list until a same prediction as the actual label is obtained\nfor input_image, input_label in data_list:\n    actual = labels[input_label.cpu()]\n    \n    model.eval()\n    output, intermediate_activations = model(input_image)\n    _, predicted = torch.max(output, 1)\n    prediction = labels[predicted.cpu()]\n    \n    if prediction == actual:\n        activation_images = [activation.squeeze().detach().cpu().numpy() for activation in intermediate_activations]\n        activation_images_resized = [np.mean(image, axis=0) for image in activation_images]\n        titles = [f'Conv Layer {i+1} Activation' for i in range(len(activation_images_resized))]\n\n        # Plot intermediate activations in a grid\n        plot_grid(activation_images_resized, titles, prediction, actual)\n        break\n\n```\n\n\n    \n![](output_26_0.png)\n    \n\n\n\n```python\nimport torch\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_grid(images, titles, prediction, actual):\n    num_images = len(images)\n    cols = min(5, num_images)\n    rows = (num_images - 1) // cols + 1\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 4))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes.flatten()[i]\n        ax.imshow(image, cmap='viridis')\n        ax.set_title(title)\n        ax.axis('off')\n    plt.suptitle(f'Prediction: {prediction}\\nActual: {actual}', fontsize=16)\n    plt.show()\n\n# Load the model from disk\nmodel_path = 'custom_cnn_binary.pth'\nmodel = ConvNet()\n\nstate_dict = torch.load(model_path)\nmodel.load_state_dict(state_dict)\n\nlabels = ['Bat', 'Others']\n\n# Get input_image tensor from test dataset\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\ndata_list = list(test_loader)\n\n# Shuffle the data list to ensure randomness\nrandom.shuffle(data_list)\n\n# Iterate through the data list until a same prediction as the actual label is obtained\nfor input_image, input_label in data_list:\n    actual = labels[input_label.cpu()]\n    \n    model.eval()\n    output, intermediate_activations = model(input_image)\n    _, predicted = torch.max(output, 1)\n    prediction = labels[predicted.cpu()]\n    \n    if prediction == actual:\n        activation_images = [activation.squeeze().detach().cpu().numpy() for activation in intermediate_activations]\n        activation_images_resized = [np.mean(image, axis=0) for image in activation_images]\n        titles = [f'Conv Layer {i+1} Activation' for i in range(len(activation_images_resized))]\n\n        # Plot intermediate activations in a grid\n        plot_grid(activation_images_resized, titles, prediction, actual)\n        break\n\n```\n\n\n    \n![](output_27_0.png)\n    \n\n\n\n```python\nimport torch\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_grid(images, titles, prediction, actual):\n    num_images = len(images)\n    cols = min(5, num_images)\n    rows = (num_images - 1) // cols + 1\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 4))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes.flatten()[i]\n        ax.imshow(image, cmap='viridis')\n        ax.set_title(title)\n        ax.axis('off')\n    plt.suptitle(f'Prediction: {prediction}\\nActual: {actual}', fontsize=16)\n    plt.show()\n\n# Load the model from disk\nmodel_path = 'custom_cnn_binary.pth'\nmodel = ConvNet()\n\nstate_dict = torch.load(model_path)\nmodel.load_state_dict(state_dict)\n\nlabels = ['Bat', 'Others']\n\n# Get input_image tensor from test dataset\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\ndata_list = list(test_loader)\n\n# Shuffle the data list to ensure randomness\nrandom.shuffle(data_list)\n\n# Iterate through the data list until a same prediction as the actual label is obtained\nfor input_image, input_label in data_list:\n    actual = labels[input_label.cpu()]\n    \n    model.eval()\n    output, intermediate_activations = model(input_image)\n    _, predicted = torch.max(output, 1)\n    prediction = labels[predicted.cpu()]\n    \n    if prediction == actual:\n        activation_images = [activation.squeeze().detach().cpu().numpy() for activation in intermediate_activations]\n        activation_images_resized = [np.mean(image, axis=0) for image in activation_images]\n        titles = [f'Conv Layer {i+1} Activation' for i in range(len(activation_images_resized))]\n\n        # Plot intermediate activations in a grid\n        plot_grid(activation_images_resized, titles, prediction, actual)\n        break\n\n```\n\n\n    <Figure size 640x480 with 0 Axes>\n\n\n\n    \n![](output_28_1.png)\n    \n\n\n### Discussing automatically created features for identification of Bat\n- The model identifies the head like structure between two triangular shaped wings\n- Head or lower body both can work provided they are between two triangular shaped wings\n- It then focusses more on the head and then on the wings\n- Eventually, as we move on from one layer to another, the focus on wings also decrease, but the focus on the head increases\n\n## Binary Classification Using AlexNet architecture (Not pre-trained)\n\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom sklearn.model_selection import KFold\nfrom torchvision.models import alexnet\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Define the AlexNet architecture\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes=2):\n        super(AlexNet, self).__init__()\n        self.features = alexnet().features\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Define transformations and create dataset\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndataset = datasets.ImageFolder('one-vs-rest/', transform=transform)\n\n# Split dataset into three folds using KFold\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\n\nall_labels = []\nall_predictions = []\n\n# Step 4: Train and evaluate the model for each fold\nfor fold, (train_index, test_index) in enumerate(kf.split(dataset)):\n    train_sampler = SubsetRandomSampler(train_index)\n    test_sampler = SubsetRandomSampler(test_index)\n    train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler)\n    test_loader = DataLoader(dataset, batch_size=32, sampler=test_sampler)\n\n    # Define model, loss function, and optimizer\n    model = AlexNet(num_classes=2)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Train the model\n    for epoch in range(num_epochs):\n        model.train()\n        for inputs, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n    # Evaluate the model on test data\n    model.eval()\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            all_labels.extend(labels.tolist())\n            all_predictions.extend(predicted.tolist())\n```\n\n\n```python\n# Compute overall classification matrix\ncm = classification_matrix(all_labels, all_predictions)\n\naccuracy = (conf_matrix.diagonal().sum() / conf_matrix.sum()) * 100\n\n# Plot the classification matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d')\nplt.title(f'Classification Matrix\\nAccuracy: {accuracy:.2f}%')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()\n```\n\n\n    \n![](output_32_0.png)\n    \n\n\n## 5-Class Classification using Custom CNN\n\n\n```python\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, Subset, random_split, SubsetRandomSampler, ConcatDataset\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import KFold\n\ntorch.manual_seed(1234)\ntorch.cuda.manual_seed(1234)\nnp.random.seed(1234)\n```\n\n### Data Augmentation\n\n\n```python\n# Tried to expand the dataset using data augmentation in order to improve accuracy as the dataset given was small\n\nbatch_size = 5\nnum_epochs = 15\nlearning_rate=0.001\n\nprint(torch.cuda.is_available())\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Augmentation transforms\naug_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    transforms.ToPILImage(),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.ToTensor(),\n])\n\n# Normal transforms\nstd_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ndataset1 = datasets.ImageFolder('five-class/', transform=std_transform)\ndataset2 = datasets.ImageFolder('five-class/', transform=aug_transform)\ndataset = ConcatDataset([dataset1, dataset2])\ntest_dataset = dataset\n```\n\n    True\n    \n\n\n```python\n# show random images after loading the data\n\ntrain_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n\ndef imshow(img):\n    # Unnormalize the image tensor\n    img = img * 0.229 + 0.485\n    npimg = img.numpy()\n    npimg = np.clip(npimg, 0, 1)\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n# get some random training images\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\nprint(labels)\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n```\n\n    tensor([4, 2, 3, 3, 4, 4, 3, 4, 4, 0, 3, 0, 1, 4, 2, 4])\n    \n\n\n    \n![](output_37_1.png)\n    \n\n\n\n```python\n# Our Model for training\n\n# Experimented with batch normalization layers to improve accuracy, there is stil room for improvement in accuracy\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=5):\n        super(ConvNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n            #nn.BatchNorm2d(64),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n            #nn.BatchNorm2d(192),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            #nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.BatchNorm1d(4096),\n            nn.ReLU(inplace=True),\n            #nn.BatchNorm1d(4096),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.BatchNorm1d(4096),\n            nn.ReLU(inplace=True),\n            \n            nn.Linear(4096, num_classes),\n        )\n        \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\nmodel = ConvNet().to(device)\n\n\n```\n\n\n```python\n# Our Model for getting the output of the convolutional layers\n\n# Experimented with batch normalization layers to improve accuracy, there is stil room for improvement in accuracy\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=5):\n        super(ConvNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n            #nn.BatchNorm2d(64),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.BatchNorm2d(192),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n            #nn.BatchNorm2d(192),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            \n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            #nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.BatchNorm1d(4096),\n            nn.ReLU(inplace=True),\n            #nn.BatchNorm1d(4096),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.BatchNorm1d(4096),\n            nn.ReLU(inplace=True),\n            \n            nn.Linear(4096, num_classes),\n        )\n        \n        self.intermediate_activations = []\n\n        # Register hooks to store intermediate activations\n        for layer in self.features:\n            if isinstance(layer, nn.Conv2d):\n                layer.register_forward_hook(self.hook_fn)\n\n    def hook_fn(self, module, input, output):\n        # Store intermediate activations\n        self.intermediate_activations.append(output)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x, self.intermediate_activations\n\nmodel = ConvNet().to(device)\n\n\n```\n\n\n```python\n# Define K-fold cross-validation\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\nepochs = 10\naccuracies = []\n\n# Iterate through each fold\nfor fold, (train_index, test_index) in enumerate(kf.split(dataset)):\n    # Split the augmented dataset into train and test sets for this fold\n    train_dataset = torch.utils.data.Subset(dataset, train_index)\n    test_dataset = torch.utils.data.Subset(dataset, test_index)\n\n    # Create DataLoader for train and test sets\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n    \n    # Define your model, loss function, and optimizer\n    model = ConvNet(num_classes=5).to(device)  # Assuming 5 output classes\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\n    # Train the model\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            #outputs, intermediate_activations = model(inputs)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)  \n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n\n        # Print training statistics\n        print(f'Fold {fold+1}, Epoch {epoch+1}, Training Loss: {running_loss / len(train_loader.dataset):.4f}')\n\n    # Test the model\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            #outputs, intermediate_activations = model(inputs)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)  # Get the predicted class with highest probability\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f'Fold {fold+1}, Test Accuracy: {accuracy:.2f}%')\n    accuracies.append(accuracy)\n\n# Final accuracy\nprint(\"Final accuracy:\", np.mean(accuracies))\n\n```\n\n    Fold 1, Epoch 1, Training Loss: 1.5476\n    Fold 1, Epoch 2, Training Loss: 1.2713\n    Fold 1, Epoch 3, Training Loss: 1.0316\n    Fold 1, Epoch 4, Training Loss: 0.9983\n    Fold 1, Epoch 5, Training Loss: 0.9220\n    Fold 1, Epoch 6, Training Loss: 0.8885\n    Fold 1, Epoch 7, Training Loss: 0.8359\n    Fold 1, Epoch 8, Training Loss: 0.7286\n    Fold 1, Epoch 9, Training Loss: 0.8208\n    Fold 1, Epoch 10, Training Loss: 0.7024\n    Fold 1, Test Accuracy: 56.00%\n    Fold 2, Epoch 1, Training Loss: 1.6129\n    Fold 2, Epoch 2, Training Loss: 1.1900\n    Fold 2, Epoch 3, Training Loss: 1.0651\n    Fold 2, Epoch 4, Training Loss: 0.9705\n    Fold 2, Epoch 5, Training Loss: 0.9372\n    Fold 2, Epoch 6, Training Loss: 0.8511\n    Fold 2, Epoch 7, Training Loss: 0.7492\n    Fold 2, Epoch 8, Training Loss: 0.7628\n    Fold 2, Epoch 9, Training Loss: 0.7357\n    Fold 2, Epoch 10, Training Loss: 0.6575\n    Fold 2, Test Accuracy: 59.00%\n    Fold 3, Epoch 1, Training Loss: 1.5582\n    Fold 3, Epoch 2, Training Loss: 1.1821\n    Fold 3, Epoch 3, Training Loss: 1.0665\n    Fold 3, Epoch 4, Training Loss: 1.0389\n    Fold 3, Epoch 5, Training Loss: 0.8886\n    Fold 3, Epoch 6, Training Loss: 0.8919\n    Fold 3, Epoch 7, Training Loss: 0.8593\n    Fold 3, Epoch 8, Training Loss: 0.8592\n    Fold 3, Epoch 9, Training Loss: 0.8539\n    Fold 3, Epoch 10, Training Loss: 0.7271\n    Fold 3, Test Accuracy: 59.50%\n    Final accuracy: 58.166666666666664\n    \n\n\n```python\n# run this to save the model with best accuracy on the disk\ntorch.save(model.state_dict(), 'custom_cnn_5_class.pth')\n```\n\n\n```python\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nnum_classes = 5\n\n# Initialize the model\nmodel = ConvNet(num_classes).to(device)\nmodel.load_state_dict(torch.load('custom_cnn_5_class.pth'))\nmodel.eval()\n\ntrue_labels = []\npred_labels = []\n\n# Iterate over the test dataset and make predictions\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        true_labels.extend(labels.cpu().numpy())\n        pred_labels.extend(predicted.cpu().numpy())\n\n# Calculate the classification matrix\nconf_matrix = confusion_matrix(true_labels, pred_labels)\n\n# Calculate accuracy using the classification matrix\naccuracy = (np.diag(conf_matrix).sum() / conf_matrix.sum()) * 100\n\n# Plot the classification matrix using seaborn heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True,fmt='d', cbar=True)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title(f'Classification Matrix\\nAccuracy: {accuracy:.2f}%')\nplt.show()\n\n```\n\n\n    \n![](output_42_0.png)\n    \n\n\n## Visualizing Output of CNN Layers and looking for Automatically created Features\n\n\n```python\nimport torch\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_grid(images, titles, rows, cols, prediction, actual):\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 4))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes.flatten()[i]\n        ax.imshow(image, cmap='viridis')\n        ax.set_title(title)\n        ax.axis('off')\n    plt.suptitle(f'Prediction: {prediction}\\nActual: {actual}',fontsize=16)\n    plt.show()\n\n# Load the model from disk\nmodel_path = 'custom_cnn_5_class.pth'\nmodel = ConvNet()\n\n# Load the state dictionary of the model\nstate_dict = torch.load(model_path)\n\n# Load the state dictionary into the model\nmodel.load_state_dict(state_dict)\nlabels=['Cat','Cow','Octopus','Oyster','Panda']\n# Get input_image tensor from test dataset\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\ndata_list = list(test_loader)\n\n# Get a random item from the list\nrandom_item = random.choice(data_list)\n\n# Extract image and label from the random item\ninput_image, input_label = random_item\nactual = labels[input_label.cpu()]\n\nmodel.eval()\noutput, intermediate_activations = model(input_image)\n_, predicted = torch.max(output, 1)\nprediction = labels[predicted.cpu()]\nwhile prediction != actual:\n    # Get a random item from the list\n    random_item = random.choice(data_list)\n    \n    # Extract image and label from the random item\n    input_image, input_label = random_item\n    actual = labels[input_label.cpu()]\n    \n    output, intermediate_activations = model(input_image)\n    _, predicted = torch.max(output, 1)\n    prediction = labels[predicted.cpu()]\n    \n# Resize activation maps to a square shape for plotting\nactivation_images = [activation.squeeze().detach().cpu().numpy() for activation in intermediate_activations]\nactivation_images_resized = [np.mean(image, axis=0) for image in activation_images]  # Compute channel-wise mean\ntitles = [f'Conv Layer {i+1} Activation' for i in range(len(activation_images_resized))]\n\n# Plot intermediate activations in a grid\nplot_grid(activation_images_resized, titles, 1, 5, prediction, actual)\n```\n\n\n    \n![](output_44_0.png)\n    \n\n\n\n```python\nimport torch\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_grid(images, titles, rows, cols, prediction, actual):\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 4))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes.flatten()[i]\n        ax.imshow(image, cmap='viridis')\n        ax.set_title(title)\n        ax.axis('off')\n    plt.suptitle(f'Prediction: {prediction}\\nActual: {actual}',fontsize=16)\n    plt.show()\n\n# Load the model from disk\nmodel_path = 'custom_cnn_5_class.pth'\nmodel = ConvNet()\n\n# Load the state dictionary of the model\nstate_dict = torch.load(model_path)\n\n# Load the state dictionary into the model\nmodel.load_state_dict(state_dict)\nlabels=['Cat','Cow','Octopus','Oyster','Panda']\n# Get input_image tensor from test dataset\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\ndata_list = list(test_loader)\n\n# Get a random item from the list\nrandom_item = random.choice(data_list)\n\n# Extract image and label from the random item\ninput_image, input_label = random_item\nactual = labels[input_label.cpu()]\n\nmodel.eval()\noutput, intermediate_activations = model(input_image)\n_, predicted = torch.max(output, 1)\nprediction = labels[predicted.cpu()]\nwhile prediction != actual:\n    # Get a random item from the list\n    random_item = random.choice(data_list)\n    \n    # Extract image and label from the random item\n    input_image, input_label = random_item\n    actual = labels[input_label.cpu()]\n    \n    output, intermediate_activations = model(input_image)\n    _, predicted = torch.max(output, 1)\n    prediction = labels[predicted.cpu()]\n    \n# Resize activation maps to a square shape for plotting\nactivation_images = [activation.squeeze().detach().cpu().numpy() for activation in intermediate_activations]\nactivation_images_resized = [np.mean(image, axis=0) for image in activation_images]  # Compute channel-wise mean\ntitles = [f'Conv Layer {i+1} Activation' for i in range(len(activation_images_resized))]\n\n# Plot intermediate activations in a grid\nplot_grid(activation_images_resized, titles, 1, 5, prediction, actual)\n```\n\n\n    \n![](output_45_0.png)\n    \n\n\n### Discussing automatically created features for identification of Panda\n- The model identifies the head and body\n- It identified facial features in the first layer such as eyes shaped as panda, nose, mputh and ears\n- As we proceed in layers, the focus on head and body increases and facial features are disappeared\n\n\n```python\nimport torch\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_grid(images, titles, rows, cols, prediction, actual):\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 4))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes.flatten()[i]\n        ax.imshow(image, cmap='viridis')\n        ax.set_title(title)\n        ax.axis('off')\n    plt.suptitle(f'Prediction: {prediction}\\nActual: {actual}',fontsize=16)\n    plt.show()\n\n# Load the model from disk\nmodel_path = 'custom_cnn_5_class.pth'\nmodel = ConvNet()\n\n# Load the state dictionary of the model\nstate_dict = torch.load(model_path)\n\n# Load the state dictionary into the model\nmodel.load_state_dict(state_dict)\nlabels=['Cat','Cow','Octopus','Oyster','Panda']\n# Get input_image tensor from test dataset\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\ndata_list = list(test_loader)\n\n# Get a random item from the list\nrandom_item = random.choice(data_list)\n\n# Extract image and label from the random item\ninput_image, input_label = random_item\nactual = labels[input_label.cpu()]\n\nmodel.eval()\noutput, intermediate_activations = model(input_image)\n_, predicted = torch.max(output, 1)\nprediction = labels[predicted.cpu()]\nwhile prediction != actual:\n    # Get a random item from the list\n    random_item = random.choice(data_list)\n    \n    # Extract image and label from the random item\n    input_image, input_label = random_item\n    actual = labels[input_label.cpu()]\n    \n    output, intermediate_activations = model(input_image)\n    _, predicted = torch.max(output, 1)\n    prediction = labels[predicted.cpu()]\n    \n# Resize activation maps to a square shape for plotting\nactivation_images = [activation.squeeze().detach().cpu().numpy() for activation in intermediate_activations]\nactivation_images_resized = [np.mean(image, axis=0) for image in activation_images]  # Compute channel-wise mean\ntitles = [f'Conv Layer {i+1} Activation' for i in range(len(activation_images_resized))]\n\n# Plot intermediate activations in a grid\nplot_grid(activation_images_resized, titles, 1, 5, prediction, actual)\n```\n\n\n    \n![](output_47_0.png)\n    \n\n\n### Discussing automatically created features for identification of Octopus\n- The model identifies the head and tentacles\n- The focus in more on head like structure between or near tentacles connected to the it\n- As we proceed in layers, the focus on head also reduces a bit\n\n\n```python\nimport torch\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_grid(images, titles, rows, cols, prediction, actual):\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 4))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes.flatten()[i]\n        ax.imshow(image, cmap='viridis')\n        ax.set_title(title)\n        ax.axis('off')\n    plt.suptitle(f'Prediction: {prediction}\\nActual: {actual}',fontsize=16)\n    plt.show()\n\n# Load the model from disk\nmodel_path = 'custom_cnn_5_class.pth'\nmodel = ConvNet()\n\n# Load the state dictionary of the model\nstate_dict = torch.load(model_path)\n\n# Load the state dictionary into the model\nmodel.load_state_dict(state_dict)\nlabels=['Cat','Cow','Octopus','Oyster','Panda']\n# Get input_image tensor from test dataset\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\ndata_list = list(test_loader)\n\n# Get a random item from the list\nrandom_item = random.choice(data_list)\n\n# Extract image and label from the random item\ninput_image, input_label = random_item\nactual = labels[input_label.cpu()]\n\nmodel.eval()\noutput, intermediate_activations = model(input_image)\n_, predicted = torch.max(output, 1)\nprediction = labels[predicted.cpu()]\nwhile prediction != actual:\n    # Get a random item from the list\n    random_item = random.choice(data_list)\n    \n    # Extract image and label from the random item\n    input_image, input_label = random_item\n    actual = labels[input_label.cpu()]\n    \n    output, intermediate_activations = model(input_image)\n    _, predicted = torch.max(output, 1)\n    prediction = labels[predicted.cpu()]\n    \n# Resize activation maps to a square shape for plotting\nactivation_images = [activation.squeeze().detach().cpu().numpy() for activation in intermediate_activations]\nactivation_images_resized = [np.mean(image, axis=0) for image in activation_images]  # Compute channel-wise mean\ntitles = [f'Conv Layer {i+1} Activation' for i in range(len(activation_images_resized))]\n\n# Plot intermediate activations in a grid\nplot_grid(activation_images_resized, titles, 1, 5, prediction, actual)\n```\n\n\n    \n![](output_49_0.png)\n    \n\n\n### Discussing automatically created features for identification of Oyster\n- The model identifies the spaces in between and looks for oyster-like shape\n- It looks for slot like structures\n- It then focusses on the actual oyster more, eliminating the background\n\n\n```python\nimport torch\nimport random\nimport matplotlib.pyplot as plt\n\ndef plot_grid(images, titles, rows, cols, prediction, actual):\n    fig, axes = plt.subplots(rows, cols, figsize=(15, 4))\n    for i, (image, title) in enumerate(zip(images, titles)):\n        ax = axes.flatten()[i]\n        ax.imshow(image, cmap='viridis')\n        ax.set_title(title)\n        ax.axis('off')\n    plt.suptitle(f'Prediction: {prediction}\\nActual: {actual}',fontsize=16)\n    plt.show()\n\n# Load the model from disk\nmodel_path = 'custom_cnn_5_class.pth'\nmodel = ConvNet()\n\n# Load the state dictionary of the model\nstate_dict = torch.load(model_path)\n\n# Load the state dictionary into the model\nmodel.load_state_dict(state_dict)\nlabels=['Cat','Cow','Octopus','Oyster','Panda']\n# Get input_image tensor from test dataset\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\ndata_list = list(test_loader)\n\n# Get a random item from the list\nrandom_item = random.choice(data_list)\n\n# Extract image and label from the random item\ninput_image, input_label = random_item\nactual = labels[input_label.cpu()]\n\nmodel.eval()\noutput, intermediate_activations = model(input_image)\n_, predicted = torch.max(output, 1)\nprediction = labels[predicted.cpu()]\nwhile prediction != actual:\n    # Get a random item from the list\n    random_item = random.choice(data_list)\n    \n    # Extract image and label from the random item\n    input_image, input_label = random_item\n    actual = labels[input_label.cpu()]\n    \n    output, intermediate_activations = model(input_image)\n    _, predicted = torch.max(output, 1)\n    prediction = labels[predicted.cpu()]\n    \n# Resize activation maps to a square shape for plotting\nactivation_images = [activation.squeeze().detach().cpu().numpy() for activation in intermediate_activations]\nactivation_images_resized = [np.mean(image, axis=0) for image in activation_images]  # Compute channel-wise mean\ntitles = [f'Conv Layer {i+1} Activation' for i in range(len(activation_images_resized))]\n\n# Plot intermediate activations in a grid\nplot_grid(activation_images_resized, titles, 1, 5, prediction, actual)\n```\n\n\n    \n![](output_51_0.png)\n    \n\n\n### Discussing automatically created features for identification of Cat\n- The model identifies the and the eyes, nose etc. (facial features) first\n- It then focusses more on the head and jaw\n- Eventually, as we move on from one layer to another, the focus on the head increases\n\n## Binary Classification using AlexNet (not pre-trained)\n\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom sklearn.model_selection import KFold\nfrom torchvision.models import alexnet\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_epochs = 10\n\n# Define the AlexNet architecture\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes=5):\n        super(AlexNet, self).__init__()\n        self.features = alexnet().features\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Define transformations and create dataset\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Update dataset directory to your 5-class dataset directory\ndataset = datasets.ImageFolder('five-class/', transform=transform)\n\n# Split dataset into three folds using KFold\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\n\nall_labels = []\nall_predictions = []\nall_accuracies = []\n\n# Train and evaluate the model for each fold\nfor fold, (train_index, test_index) in enumerate(kf.split(dataset)):\n    train_sampler = SubsetRandomSampler(train_index)\n    test_sampler = SubsetRandomSampler(test_index)\n    train_loader = DataLoader(dataset, batch_size=16, sampler=train_sampler)\n    test_loader = DataLoader(dataset, batch_size=16, sampler=test_sampler)\n\n    # Define model, loss function, and optimizer\n    model = AlexNet().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n    # Training loop\n    for epoch in range(num_epochs):  # You need to define num_epochs\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * inputs.size(0)\n        print(f'Fold {fold+1}, Epoch {epoch+1}, Training Loss: {running_loss / len(train_loader.dataset):.4f}')\n\n    # Evaluate the model on test data after training\n    model.eval()\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            all_labels.extend(labels.tolist())\n            all_predictions.extend(predicted.tolist())\n    \n    # Calculate accuracy for the current fold\n    accuracy = accuracy_score(all_labels, all_predictions)\n    all_accuracies.append(accuracy)\n    print(f'Fold [{fold + 1}] Accuracy: {accuracy:.4f}')\n\n# Calculate final accuracy across all folds\nfinal_accuracy = np.mean(all_accuracies)\nprint(f'Final Accuracy: {final_accuracy:.4f}')\n```\n\n    Fold 1, Epoch 1, Training Loss: 1.0693\n    Fold 1, Epoch 2, Training Loss: 1.0204\n    Fold 1, Epoch 3, Training Loss: 0.9608\n    Fold 1, Epoch 4, Training Loss: 0.8554\n    Fold 1, Epoch 5, Training Loss: 0.9104\n    Fold 1, Epoch 6, Training Loss: 0.8728\n    Fold 1, Epoch 7, Training Loss: 0.7820\n    Fold 1, Epoch 8, Training Loss: 0.7446\n    Fold 1, Epoch 9, Training Loss: 0.6400\n    Fold 1, Epoch 10, Training Loss: 0.6361\n    Fold [1] Accuracy: 0.3800\n    Fold 2, Epoch 1, Training Loss: 1.0755\n    Fold 2, Epoch 2, Training Loss: 1.0533\n    Fold 2, Epoch 3, Training Loss: 1.0308\n    Fold 2, Epoch 4, Training Loss: 0.9636\n    Fold 2, Epoch 5, Training Loss: 0.8872\n    Fold 2, Epoch 6, Training Loss: 0.8367\n    Fold 2, Epoch 7, Training Loss: 0.8187\n    Fold 2, Epoch 8, Training Loss: 0.9069\n    Fold 2, Epoch 9, Training Loss: 0.7060\n    Fold 2, Epoch 10, Training Loss: 0.6765\n    Fold [2] Accuracy: 0.4600\n    Fold 3, Epoch 1, Training Loss: 1.0750\n    Fold 3, Epoch 2, Training Loss: 1.0664\n    Fold 3, Epoch 3, Training Loss: 1.0445\n    Fold 3, Epoch 4, Training Loss: 0.9687\n    Fold 3, Epoch 5, Training Loss: 0.9161\n    Fold 3, Epoch 6, Training Loss: 0.8616\n    Fold 3, Epoch 7, Training Loss: 0.8181\n    Fold 3, Epoch 8, Training Loss: 0.8172\n    Fold 3, Epoch 9, Training Loss: 0.7912\n    Fold 3, Epoch 10, Training Loss: 0.6597\n    Fold [3] Accuracy: 0.4967\n    Final Accuracy: 0.4456\n    \n\n\n```python\n# Calculate and plot confusion matrix\nconf_matrix = confusion_matrix(all_labels, all_predictions)\n\n# Calculate accuracy using the classification matrix\naccuracy = (np.diag(conf_matrix).sum() / conf_matrix.sum()) * 100\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cbar=True)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title(f'Classification Matrix\\nAccuracy: {accuracy:.2f}%')\nplt.show()\n```\n\n\n    \n![](output_55_0.png)\n    \n\n\n# Thank You!\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"Task.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","theme":"cosmo","title-block-banner":true,"title":"Binary and Multi-Class Classification in Python","description":"The blog post created using the jupyter notebook file and Quarto","author":"Suvid Singhal","categories":["Deep Learning"],"date":"5/22/2021","draft":false},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}